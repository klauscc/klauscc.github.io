<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Feng Cheng]]></title>
  <link href="klauscc.github.io/atom.xml" rel="self"/>
  <link href="klauscc.github.io/"/>
  <updated>2022-06-21T01:47:34-07:00</updated>
  <id>klauscc.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Feng Cheng]]></title>
    <link href="klauscc.github.io/Feng_Cheng.html"/>
    <updated>2018-10-21T20:23:35-07:00</updated>
    <id>klauscc.github.io/Feng_Cheng.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">About me</h2>

<p>I am currently a Ph.D student in Department of Computer Science at UNC at Chapel Hill, advised by Prof. <a href="https://www.gedasbertasius.com">Gedas Bertasius</a>. My current research interests lie in computer vision. Specifically, I&#39;m interested in video understanding, efficiency of video models and multi-modality.</p>

<p>Prior to joining Gedas&#39;s Group, I worked with Prof. <a href="https://scholar.google.com/citations?user=v6VYQC8AAAAJ&amp;hl=en">Dinggang Shen</a> and Prof. <a href="https://scholar.google.com/citations?user=QGdnthwAAAAJ&amp;hl=en">Pew-Thian Yap</a> at UNC on medical imaging. I obtained both my B.S. degree in Information Security in 2016 and M.S. degree in ECE in 2019 at Shanghai JiaoTong University. During my M.S. study, I worked with Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=8tg3mv0AAAAJ&amp;view_op=list_works">Shilin Wang</a>.</p>

<h2 id="toc_1">Publications</h2>

<ul>
<li><p><a href="https://arxiv.org/abs/2204.01680">TALLFormer: Temporal Action Localization with Long-memory Transformer</a> <br/>
<strong>Feng Cheng</strong>, Gedas Bertasius <br/>
<strong>Preprint</strong> [<a href="https://github.com/klauscc/TALLFormer">Code</a>]</p></li>
<li><p><a href="https://arxiv.org/abs/2203.16755">Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models</a> <br/>
<strong>Feng Cheng</strong>, Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Li, Wei Xia<br/>
<strong>CVPR 2022</strong> (<span style="color:red"><strong>Oral</strong></span>) [<a href="https://github.com/amazon-research/stochastic-backpropagation">Code</a>]</p></li>
<li><p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Spatio-Temporal_Fusion_Based_Convolutional_Sequence_Learning_for_Lip_Reading_ICCV_2019_paper.pdf">Spatio-Temporal Fusion based Convolutional Sequence Learning for Lip Reading</a> <br/>
Xingxuan Zhang, <strong>Feng Cheng</strong>, and Shi-Lin Wang <br/>
<strong>ICCV 2019</strong></p></li>
<li><p><a href="https://www.ismrm.org/21/program-files/TeaserSlides/TeasersPresentations/0170-Teaser.html">Submillimeter 3D MR Fingerprinting with Whole-Brain Coverage via Dual-Domain Deep Learning Reconstruction</a> <br/>
<strong>Feng Cheng</strong>, Yong Chen and Pew-Thian Yap <br/>
<strong>ISMRM 2021</strong>. (<span style="color:red"><strong>Oral</strong></span>, “magna cum laude” award)</p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-030-59713-9_16">Acceleration of High-Resolution 3D MR Fingerprinting via a Graph Convolutional Network</a> <br/>
<strong>Feng Cheng</strong>, Yong Chen, Xiaopeng Zong, Weili Lin, Pew-Thian Yap,<br/>
Dinggang Shen<br/>
<strong>MICCAI 2020</strong></p></li>
<li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320318302152">Visual speaker authentication with random prompt texts by a dual-task CNN framework</a><br/>
<strong>Feng Cheng</strong>, Shi-Lin Wang, and Alan Wee-Chung Liew.<br/>
<strong>Pattern Recognition 2018</strong></p></li>
</ul>

<h2 id="toc_2">Experiences</h2>

<ul>
<li><strong>2022 /05 ~ 2022 /08</strong> Applied Scientist Intern at Amazon AWS AI 
Mentor: <a href="https://scholar.google.com/citations?user=c8WhHZkAAAAJ&amp;hl=en">Bing Shuai</a> &amp; <a href="https://scholar.google.com/citations?user=KNcECJQAAAAJ&amp;hl=en">Mingze Xu</a></li>
<li><strong>2021 /05 ~ 2021 /08</strong> Applied Scientist Intern at Amazon AWS AI
Mentor: <a href="https://scholar.google.com/citations?user=KNcECJQAAAAJ&amp;hl=en">Mingze Xu</a> &amp; <a href="https://scholar.google.com/citations?user=ojKsx6AAAAAJ&amp;hl=en">Yuanjun Xiong</a></li>
</ul>

<h2 id="toc_3">Honors</h2>

<ul>
<li><strong>2016</strong> <strong>First prize</strong> of the Ninth National Information Security Competition</li>
<li><strong>2016</strong> <strong>Second Prize</strong> of the 13th Huawei Cup National Graduate Mathematical Modeling Contest</li>
<li><strong>2017</strong> Wenyuan Pan scholarship (<strong>7000 RMB</strong>)</li>
<li><strong>2018</strong> National Cyberspace safety scholarship(<strong>50000 RMB</strong>)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Techniques in convolution]]></title>
    <link href="klauscc.github.io/convs.html"/>
    <updated>2019-05-08T02:48:34-07:00</updated>
    <id>klauscc.github.io/convs.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1. Convolution Variants</a>
<ul>
<li>
<a href="#toc_1">1.1 Normal Convolution</a>
</li>
<li>
<a href="#toc_2">1.2 Depth-wise Convolution and Point-wise Convolution</a>
</li>
<li>
<a href="#toc_3">1.3 Deformable Convolution</a>
</li>
<li>
<a href="#toc_4">1.4 Dynamic Convolution</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">2. Classic Blocks</a>
<ul>
<li>
<a href="#toc_6">1. ResNet: Residual-Block</a>
</li>
<li>
<a href="#toc_7">2. DenseNet: Dense-Block</a>
</li>
<li>
<a href="#toc_8">3. SENet: SE-Block</a>
</li>
<li>
<a href="#toc_9">4. MobileNet: Dw-conv &amp; Pw-conv</a>
</li>
</ul>
</li>
</ul>


<p>performance: ResNet -&gt; DenseNet -&gt; SENet</p>

<p>speed: </p>

<h2 id="toc_0">1. Convolution Variants</h2>

<h3 id="toc_1">1.1 Normal Convolution</h3>

<h3 id="toc_2">1.2 Depth-wise Convolution and Point-wise Convolution</h3>

<h3 id="toc_3">1.3 Deformable Convolution</h3>

<h3 id="toc_4">1.4 Dynamic Convolution</h3>

<h2 id="toc_5">2. Classic Blocks</h2>

<h3 id="toc_6">1. ResNet: Residual-Block</h3>

<h3 id="toc_7">2. DenseNet: Dense-Block</h3>

<h3 id="toc_8">3. SENet: SE-Block</h3>

<h3 id="toc_9">4. MobileNet: Dw-conv &amp; Pw-conv</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stream摄像头内容到浏览器]]></title>
    <link href="klauscc.github.io/15260272539482.html"/>
    <updated>2018-05-11T01:27:33-07:00</updated>
    <id>klauscc.github.io/15260272539482.html</id>
    <content type="html"><![CDATA[
<p><strong>需求</strong>： 将网络摄像头的流截取下来，进行处理如人脸识别等以后，重新发布到浏览器中直接通过浏览器访问。</p>

<p>本文采用了一个简单的容易实现的方案，使用java语言springboot 框架实现，其它语言应该也是类似。示例代码<a href="https://github.com/klauscc/webcamera-stream-demo">webcamera-stream-demo</a></p>

<hr/>

<h2 id="toc_0">1. 实现思路</h2>

<p>使用websocket，客户端和服务器端建立一个长连接，服务器端以固定时间间隔发送一帧(一帧图片的jpeg的base64编码)给前端通过<code>&lt;img&gt;</code>展示。具体流程:</p>

<ol>
<li>通过opencv 读取摄像头的RTSP视频流。RTSP协议被大多数网络摄像头支持，摄像头的RTSP流的url可以搜索一下<code>各主流摄像头的rtsp地址格式</code></li>
<li>对读取的帧进行想要的操作</li>
<li>通过websocket发布操作后的帧</li>
</ol>

<h2 id="toc_1">2. 具体实现</h2>

<ul>
<li><code>PreviewController.java</code> <code>greeting()</code>方法会每隔50ms(20fps)调用一次，每次将最新的一帧发布出去</li>
<li><code>Camera.java</code> 创建一个后台进程不断读取摄像头的视频流</li>
<li><code>resources/static/index.html(app.js)</code> 客户端订阅socket接收帧，每接收到一帧就更新一次图片，20fps可以达到流畅的视频效果。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux入门与服务器使用指南]]></title>
    <link href="klauscc.github.io/linux-starter-about-lib-and-conda.html"/>
    <updated>2017-11-14T21:26:35-08:00</updated>
    <id>klauscc.github.io/linux-starter-about-lib-and-conda.html</id>
    <content type="html"><![CDATA[
<p>深度学习的体系依托于python,在linux系统繁华，希望从事深度学习研究非常有必要对linux有一个很好的了解。</p>

<p><strong>本教程并非一个从零开始或者完全指南，你需要以下能力:</strong></p>

<pre><code>1. 了解linux 文件结构
2. 能使用命令行在各个路径下自由切换，查看文件夹和文件属性等
3. 了解什么是环境变量. 
4. 了解两个重要的文件的作用： /etc/profile, ~/.bashrc
5. 使用过pip安装package,在自己代码中使用过自己安装的package
</code></pre>

<p>看完本教程你将会掌握一下能力</p>

<pre><code>1. 在无root权限的情况下自由的使用服务器
2. 自己编译软件，拥有不google也可以解决依赖各种不对应的问题的能力。
3. 使用conda管理自己的环境，管理自己的依赖包
</code></pre>

<span id="more"></span><!-- more -->

<hr/>

<ul>
<li>任何包都可以<strong>不使用root安装</strong>(不考虑与系统密切相关的包)。</li>
<li>没有完整看过<a href="https://conda.io/docs/user-guide/index.html">Conda官方参考文档</a> 必须阅读第四章。</li>
<li>需要自己编译安装软件的,比如caffe，py-faster-rcnn等，必须阅读第三章。</li>
</ul>

<p><strong>内容较多，请按需阅读。</strong></p>

<ul>
<li>
<a href="#toc_0">1. 常用命令</a>
<ul>
<li>
<a href="#toc_1">1.0 必须掌握</a>
<ul>
<li>
<a href="#toc_2">1.0.1 命令</a>
</li>
<li>
<a href="#toc_3">1.0.2 环境变量</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">1.1 翻墙</a>
</li>
<li>
<a href="#toc_5">1.2 权限</a>
</li>
<li>
<a href="#toc_6">1.3 访问服务器</a>
<ul>
<li>
<a href="#toc_7">1.3.1 Windows</a>
</li>
<li>
<a href="#toc_8">1.3.2 Linux, Mac</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">1.4 服务器间传输文件</a>
<ul>
<li>
<a href="#toc_10">1.4.1 从远程服务器拉到本地</a>
</li>
<li>
<a href="#toc_11">1.4.2 从本地推到远程服务器</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_12">2. 安装软件</a>
</li>
<li>
<a href="#toc_13">3.头文件和链接库</a>
<ul>
<li>
<a href="#toc_14">3.1 Compile Time</a>
<ul>
<li>
<a href="#toc_15">3.1.1 <strong>那如果我想要编译的包的依赖不在上面的路径中怎么办？</strong></a>
</li>
<li>
<a href="#toc_16">3.1.2 SEARCH PATH order</a>
</li>
</ul>
</li>
<li>
<a href="#toc_17">3.2 Run Time</a>
<ul>
<li>
<a href="#toc_18">3.2.1 Run Time Search Path Order</a>
</li>
</ul>
</li>
<li>
<a href="#toc_19">3.3 有用的命令</a>
</li>
</ul>
</li>
<li>
<a href="#toc_20">4. conda环境管理</a>
<ul>
<li>
<a href="#toc_21">4.1 管理环境</a>
</li>
<li>
<a href="#toc_22">4.2 使用环境</a>
</li>
<li>
<a href="#toc_23">4.3 创建环境</a>
</li>
<li>
<a href="#toc_24">4.4 安装卸载package</a>
</li>
<li>
<a href="#toc_25">4.5 目录结构</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1. 常用命令</h2>

<h3 id="toc_1">1.0 必须掌握</h3>

<h4 id="toc_2">1.0.1 命令</h4>

<p>具体使用方法自行百度或google.<br/>
- <code>cd, ls, mkdir</code><br/>
- <code>find</code> 配合grep 查找文件很方便<br/>
- <code>locate</code> 直接定位文件位置,速度极快，系统将所有文件建立了一个索引，所以新增的文件需要一段时间才可以locate到. 如<code>locate libhdf5.so</code><br/>
- <code>which</code> 查找命令的完整路径. 如<code>which top</code>会输出<code>/usr/bin/top</code><br/>
- <code>grep</code> 可以过滤输出流的文本<br/>
- <code>top</code> 查看正在运行的进程<br/>
- <code>nvidia-smi</code> 查看显卡占用情况<br/>
- <code>df -h</code> 查看磁盘挂载和已用情况<br/>
- <code>screen</code> 或者<code>tmux</code> 在这些窗口中运行的任务，即使关掉你的终端也不会被杀掉，否则一旦ssh连接关闭任务会被杀死。</p>

<h4 id="toc_3">1.0.2 环境变量</h4>

<p><code>/etc/profile</code>(root才有写权限)和<code>~/.bashrc</code>是每次通过<code>ssh</code>登陆服务器都会运行的脚本。运行顺序是 <code>/etc/profile</code>-&gt;<code>~/.bashrc</code>。<br/>
一些希望每次登陆后都生效的环境变量可以放在<code>~/.bashrc</code>里面。</p>

<ul>
<li><code>PATH</code>: 额外的可执行文件搜索路径</li>
<li><code>CPATH</code>: 额外的头文件搜索路径</li>
<li><code>LD_LIBRARY_PATH</code>: 额外的RunTime 搜索路径</li>
<li>上面的路径都会添加到系统标准路径之前。 可以<code>vim /etc/profile</code>查看示例。</li>
</ul>

<h3 id="toc_4">1.1 翻墙</h3>

<p><code>proxychains4 your-command</code>. 比如<code>proxychains4 curl www.google.com</code>.服务器上翻墙免费提供，<strong>如果想其他平台linux/macOs/windows/ios/android等翻墙，欢迎赞助</strong>。</p>

<h3 id="toc_5">1.2 权限</h3>

<p><code>ll</code>, <code>chmod</code><br/>
 <a href="http://cn.linux.vbird.org/linux_basic/0210filepermission.php">鸟哥的linux私房菜,第六章linux的文件权限与目录配置</a></p>

<h3 id="toc_6">1.3 访问服务器</h3>

<h4 id="toc_7">1.3.1 Windows</h4>

<p>安装<a href="https://mobaxterm.mobatek.net/">MobaXterm</a>, 自带x11图形转发，使用会产生图形界面的命令就像本地显示屏一样弹出图形界面。</p>

<h4 id="toc_8">1.3.2 Linux, Mac</h4>

<p>命令行<code>ssh -p port username@host-ip</code>, 推荐添加公钥访问<a href="https://www.cnblogs.com/Percy_Lee/p/5698603.html">使用ssh公钥实现免密访问</a>. Mac 需要<strong>安装X11</strong>才能进行x11转发，Ubuntu自带。 </p>

<p>*<strong>uix平台建议添加config文件，这样登陆远程服务器不需要冗长的命令。</strong></p>

<p>添加文件<code>~/.ssh/config</code>,添加内容类似下面</p>

<pre><code>HOST ss2
    HostName wsl2
    #or ip
    #HostName 192.168.1.140
    Port 10010
    User klaus
    ServerAliveInterval 30
HOST ss1
    HostName wsl1
    Port 10086
    User klaus
    ServerAliveInterval 30
</code></pre>

<p>添加以后命令行输入<code>ssh ss1</code>即可登陆wsl1,账号是klaus. 使用<code>ssh other-username@ss1</code>则以other-username登陆。</p>

<h3 id="toc_9">1.4 服务器间传输文件</h3>

<h4 id="toc_10">1.4.1 从远程服务器拉到本地</h4>

<p><code>[ ]</code>中内容表示可有可无，传输文件夹需要<code>-r</code></p>

<ul>
<li>单个文件推荐 <code>scp [-r] -P port username@host-ip:/path/to/your/file /save/to/here</code></li>
<li>大量文件推荐，可端点续传 <code>sync -avz -P -e &quot;ssh -p $portNumber&quot; user@remoteip:/path/to/files/ /local/path/</code></li>
</ul>

<h4 id="toc_11">1.4.2 从本地推到远程服务器</h4>

<ul>
<li>单个文件推荐 <code>scp [-r] -P port /your/data/path username@host-ip:/path/to/save</code></li>
<li>大量文件推荐，可端点续传 <code>rsync -avz -P -e &quot;ssh -p $portNumber&quot;  /local/path/ user@remoteip:/path/to/files/</code></li>
</ul>

<p><strong>在wsl1(192.168.1.139)和wsl2(192.168.1.140)之间传输数据,ip地址请写内网ip，或者写<code>wsl1</code>或者<code>wsl2</code>，服务器会自动解析到对应ip。内网间传输文件可以达到100MB/S</strong></p>

<h2 id="toc_12">2. 安装软件</h2>

<p>linux下一般称软件为包(package). 安装方法有包管理器安装和手动编译。安装以后的东西主要分为三个部分:</p>

<ul>
<li>可执行文件. <strong>一般保存在<code>/**/bin</code>下</strong></li>
<li>头文件. c/c++的头文件，一般是作为别的package的依赖。<strong>一般保存在<code>/**/include</code>下</strong></li>
<li>链接库. 动态链接库.so和静态链接库.a. 一般作为别的package的依赖.<strong>一般保存在<code>/**/lib</code>或者<code>/**/lib64</code>下</strong></li>
</ul>

<h2 id="toc_13">3.头文件和链接库</h2>

<p><strong>头文件</strong>一般是c/c++的头文件,后缀名为<code>.h,.hxx,.hpp</code>等.<br/>
<strong>链接库</strong>分为动态链接库和静态链接库,linux上后缀名分别为<code>.so</code>和<code>.a</code>,实质上是<strong>一堆编译后的c/cpp文件(.o)打包成.so或.a</strong>。</p>

<p><strong>静态链接库.a会被嵌入到编译的程序中，但是动态链接库不会，需要在运行时找到.so文件才能运行。</strong></p>

<p>一般的软件都会依赖很多package(头文件和链接库)。编译软件时称为<strong>Compile Time</strong>, 运行软件时称为<strong>Run Time</strong>.在 <strong>Compile Time</strong>需要让编译器找到它所依赖的头文件和链接库，在<strong>Run Time</strong>需要让可执行文件找到它所以来的动态链接库。<strong>Compile Time</strong>和<strong>Run Time</strong>寻找依赖的路径是<strong>不一样</strong>的，也就是说<strong>即时编译通过，在运行的时候也会存在找不到动态链接库的情况，一般会报<code>no defined reference to ...</code>错误</strong>。</p>

<h3 id="toc_14">3.1 Compile Time</h3>

<p>在我们服务器上,查看头文件和链接库的<code>search path</code>, run <code>cpp -v /dev/null -o /dev/null</code>。编译器会根据<code>search path</code>从头一个一个的查找对应的头文件和链接库。也就是说如果安装了同一个package的不同版本，出现在前面的会覆盖掉后面的。</p>

<pre><code>#include &lt;...&gt; search starts here:
 /usr/local/cudnn-6.0/include
 /usr/lib/gcc/x86_64-redhat-linux/4.8.5/include
 /usr/local/include
 /usr/include
LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/4.8.5/:
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/:
/lib/../lib64/:
/usr/lib/../lib64/:
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../:
/lib/:
/usr/lib/
</code></pre>

<p>上面的目录分别是编译器寻找头文件和链接库的<code>search path</code>，寻找顺序按照路径先后顺序。</p>

<h4 id="toc_15">3.1.1 <strong>那如果我想要编译的包的依赖不在上面的路径中怎么办？</strong></h4>

<p>有两种方法:</p>

<ol>
<li><p><strong>编译选项添加gcc/g++的Flags, <code>-I</code>, <code>-L</code></strong>.<br/>
<code>-I</code>添加寻找头文件的目录，<code>-L</code>添加寻找链接库的目录，这两个flag可以添加多个。<br/>
如 <code>g++ test.cpp -lmylib -L/path/to/your/lib/dir -I/path/to/your/include/dir -I/path/to/your/include/dir2</code>,这一行命令告诉编译器去<code>/path/to/your/include/dir</code>和<code>/path/to/your/include/dir2</code>目录下找头文件，去<code>/path/to/your/lib/dir</code>找链接库，如果没找到就会去系统标准的<code>search path</code>中去找。 <code>-l</code>告诉编译器去寻找的动态链接库的名字，如<code>-lmylib</code>会去寻找名称为<code>libmylib.so</code>或者<code>libmylib.a</code>链接库(lib + mylib + .so/.a)。需要寻找的头文件的名称已在源代码的<code>#include &quot;&quot;</code>中指定了。</p></li>
<li><p><strong>添加环境变量 <code>CPATH</code>和<code>LIBRARY_PATH</code></strong> <br/>
<code>CPATH</code>是指定额外头文件的<code>search path</code>，多个目录以冒号<code>:</code>分隔。<br/>
<code>LIBRARY_PATH</code>是指定额外lib的<code>search path</code>,多个目录以冒号<code>:</code>分隔。<strong>特别需要注意的是，在64位系统上，每一个添加的目录，系统默认编译的gcc/g++会添加两个目录，<code>${your-dir}/../lib64</code>和<code>${your-dir}</code>。并且，对于所有目录<code>lib64</code>会位于<code>lib</code>之前,就是说所有<code>${any-prefix}/lib</code>目录都会位于所有<code>${any-prefix}/lib64</code>之后。</strong><br/>
<code>CPATH</code>和<code>LIBRARY_PATH</code>会添加到<code>-I,-L</code>添加的目录之后，系统标准目录之前。<br/>
例如添加<code>/usr/local/lib</code></p></li>
</ol>

<pre><code>export LIBRARY_PATH=/usr/local/lib
#如果LIBRARY_PATH不为空, 命令行输入echo $LIBRARY_PATH 打印不为空
#加上一个:$LIBRARY_PATH会在原有$LIBRARY_PATH前面再加上你想加的路径
export LIBRARY_PATH=/usr/local/lib:$LIBRARY_PATH
</code></pre>

<p>添加之后<code>search path</code>变为</p>

<pre><code>LIBRARY_PATH=/usr/local/lib/../lib64/:
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/:
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/:
/lib/../lib64/:
/usr/lib/../lib64/:
/usr/local/lib/:
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../:
/lib/:
/usr/lib/

#/usr/local/lib64排在第一位，但是/usr/local/lib在/lib64,/usr/lib64之后
</code></pre>

<h4 id="toc_16">3.1.2 SEARCH PATH order</h4>

<p>现在我们有<code>-I,-L</code>,<code>CPATH, LIBRARY_PATH</code>两种方式添加额外的path，还有一个后面会讲的<code>LD_LIBRARY_PATH</code>，这三种方法都会对编译器的search path产生影响。<br/>
假设 </p>

<pre><code>    LD_LIBRARY_PATH=dirlist
    ld ... -Lpath1 ... -Lpathn ...
    LIBRARY_PATH=libpathlist
</code></pre>

<p>那么search path 顺序为:</p>

<pre><code>    # -L &gt; LD_LIBRARY_PATH &gt; LIBRARY_PATH
    path1 ... pathn dirlist libpathlist
</code></pre>

<h3 id="toc_17">3.2 Run Time</h3>

<p>Run Time 即为程序运行加载的时候。Compile Time中<code>-L</code>和<code>LIBRARY_PATH</code>不会对Run Time 产生任何影响。也就是说，在运行程序的时候<code>-L</code>或者<code>LIBRARY_PATH</code>指定的额外路径如果没有被添加到runtime的search path里去，那么程序将会找不到对应的lib！</p>

<h4 id="toc_18">3.2.1 Run Time Search Path Order</h4>

<ol>
<li>rpath. 编译时指定的路径，如<code>g++ -o hello hello.c -Wl,-rpath,/usr/local/lib/hello/B,-rpath,/usr/local/lib/hello/A</code></li>
<li>LD_LIBRARY_PATH. 该环境变量中指定的路径。<strong>它的一个副作用是它的路径会影响编译器编译的路径，见3.1.2</strong></li>
<li><p>/etc/ld.so.conf 中指定的路径. 通过<code>ldconfig -v 2&gt;/dev/null | grep -v ^$&#39;\t&#39;</code>查看 <br/>
<code><br/>
(klaus_all) [klaus@wsl1 build]$ ldconfig -v 2&gt;/dev/null | grep -v ^$&#39;\t&#39;<br/>
/usr/lib64/dyninst:<br/>
/usr/lib64/iscsi:<br/>
/usr/lib64/mysql:<br/>
/usr/lib64/qt-3.3/lib:<br/>
/lib:<br/>
/lib64:<br/>
/lib/sse2: (hwcap: 0x0000000004000000)<br/>
/lib64/sse2: (hwcap: 0x0000000004000000)<br/>
/lib64/tls: (hwcap: 0x8000000000000000)<br/>
</code></p></li>
<li><p>/lib64,/usr/lib64,/lib,/usr/lib. 默认路径。</p></li>
</ol>

<p>一般的程序的rpath只有当前程序所在目录，所以如果你依赖了3和4中没有的路径的,你需要额外指定<code>LD_LIBRARY_PATH</code>, 比如 <code>export LD_LIBRARY_PATH=/some/path:$LD_LIBRARY_PATH</code>,为了方便可以将这一句添加到<code>~/.bashrc</code>中。</p>

<h3 id="toc_19">3.3 有用的命令</h3>

<p>弄清楚了Compile Time 和Run Time 的search path, 在编译程序，运行程序中遇到的问题基本可以自己解决了。有几个常用的命令可以帮助定位问题.</p>

<ul>
<li>ldd. </li>
</ul>

<p>查看一个lib或者executable所依赖的.so和其link的路径。如</p>

<pre><code>(klaus_all) [klaus@wsl1 build]$ ldd libextract_dct.so
    linux-vdso.so.1 =&gt;  (0x00007ffd9876c000)
    libjpeg.so.9 =&gt; /home/klaus/.conda/envs/klaus_all/lib/libjpeg.so.9 (0x00007fe23df50000)
    libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fe23db72000)
    /lib64/ld-linux-x86-64.so.2 (0x00007fe23e38f000)
</code></pre>

<ul>
<li>nm.</li>
</ul>

<p>列出一个lib或者executable定义的所有symbol.如</p>

<pre><code>(klaus_all) [klaus@wsl1 build]$ nm libextract_dct.so
0000000000202088 B __bss_start
0000000000202088 b completed.6337
                 w __cxa_finalize@@GLIBC_2.2.5
...skipped...
                 U jpeg_finish_decompress@@LIBJPEG_9.0
                 U jpeg_read_coefficients@@LIBJPEG_9.0
                 U jpeg_read_header@@LIBJPEG_9.0
                 U jpeg_save_markers@@LIBJPEG_9.0
                 U jpeg_std_error@@LIBJPEG_9.0
                 U jpeg_stdio_src@@LIBJPEG_9.0
                 w _Jv_RegisterClasses
                 U malloc@@GLIBC_2.2.5
0000000000000b18 T read_jpeg
0000000000000a60 t register_tm_clones
0000000000202088 d __TMC_END__
</code></pre>

<ul>
<li>readelf -d.</li>
</ul>

<p>列出ELF文件的信息,可以用来查看rpath. 如</p>

<pre><code>(klaus_all) [klaus@wsl1 build]$ readelf -d libextract_dct.so

Dynamic section at offset 0x1de8 contains 27 entries:
  Tag        Type                         Name/Value
 0x0000000000000001 (NEEDED)             Shared library: [libjpeg.so.9]
 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]
 0x000000000000000e (SONAME)             Library soname: [libextract_dct.so]
 0x000000000000000f (RPATH)              Library rpath: [/home/klaus/.conda/envs/klaus_all/lib/libjpeg.so:/home/klaus/.conda/envs/klaus_all/lib]
 0x000000000000000c (INIT)               0x918
...skipped...
 0x000000006ffffff9 (RELACOUNT)          3
 0x0000000000000000 (NULL)               0x0
</code></pre>

<h2 id="toc_20">4. conda环境管理</h2>

<p><a href="https://conda.io/docs/user-guide/tasks/manage-environments.html">conda</a>是python的一个包管理器，但是它<strong>不仅仅是</strong>一个python的包管理器。</p>

<ul>
<li>环境管理。你可以创建任意多的环境，每个环境可以安装不同的包，不同的版本，每个环境之间互相不影响。</li>
<li><strong>它可以安装非python包</strong>,如<code>tmux</code>, <code>cmake</code>, <code>hdf5</code>等等.需要什么可以先去google一下,比如搜索&quot;tmux conda&quot;，或者直接去<a href="anaconda.org">Anaconda Cloud</a>搜索是否有对应的包。</li>
<li>兼容pip. <strong>可以使用pip安装python包而不需要root权限。</strong>推荐首先查看conda上是否有这个包，如果没有再使用pip安装。</li>
</ul>

<h3 id="toc_21">4.1 管理环境</h3>

<p>服务器上配置好了几个环境:<br/>
    - <code>root</code>. 安装好miniconda后干净的环境，仅含有必要的几个包。<br/>
    - <code>common</code>. 常用的几个python包，编译caffe所有需要的依赖也在common中装好了。没有安装任何框架。<br/>
    - <code>dl-2.7</code>. 装好了keras, tensorflow, pytorch等深度学习框架。python版本为2.7</p>

<p>这两个环境只有<code>root</code>用户有权限更改（安装,删除package）,但是所有用户都有使用权限(read)。</p>

<h3 id="toc_22">4.2 使用环境</h3>

<ul>
<li>查看环境. <code>conda env list</code>. 查看所有的环境,其它用户的环境不可见。</li>
<li>查看当前环境中安装的包. <code>conda list</code>. 加上<code>grep</code>可以</li>
<li>激活环境. <code>source activate env-name</code>. env-name为环境名称如common, dl-2.7, 下同。</li>
<li>停止环境. <code>source deactivate env-name</code></li>
</ul>

<p>如果某个环境会经常</p>

<h3 id="toc_23">4.3 创建环境</h3>

<ul>
<li><code>conda create -n env-name</code> 创建一个名为env-name的环境，</li>
<li><code>conda create -n env-name numpy scipy</code> 创建一个包含numpy和scipy包的环境</li>
<li><code>conda create -n env-name scipy=0.15.0</code> 指定package的版本</li>
<li><code>conda create -n env-name --clone cloned-env</code> 克隆cloned-env到env-name中，所有cloned-env中的包都会被克隆岛env-name中。</li>
</ul>

<p><strong>自己创建的环境有写权限，你可以对它做任何事情</strong></p>

<h3 id="toc_24">4.4 安装卸载package</h3>

<ul>
<li>conda install package-name 安装</li>
<li>conda remove package-name 卸载</li>
</ul>

<p>conda 自带了几个default channel(类似仓库，每个仓库里有很多package), 但有很多额外的channel，使用上面安装命令找不到的包在其它channel可能会找到。可以去google <code>package-name conda</code> 或者<a href="anaconda.org">Anaconda Cloud</a>搜索是否存在。</p>

<h3 id="toc_25">4.5 目录结构</h3>

<p><code>root</code>创建的环境都会被安装到<code>$CONDA_INSTALL_PATH/envs</code>下，其它用户创建的环境会被安装到<code>~/.conda/envs</code>下.</p>

<p>每个环境下有三个重要的目录:<br/>
- bin<br/>
- include<br/>
- lib</p>

<p>这对应了第二章的三部分。使用conda安装的bin会默认添加到<code>PATH</code>里去(可以在命令行直接运行)。<strong>include 和 lib文件夹下的东西不会添加到CompileTime/RunTime 的 search path里去</strong>，所以你需要编译运行什么软件可以将这两个目录分别添加到<code>CPATH</code>和<code>LD_LIBRARY_PATH</code>里去。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lvm cache配置 -- 用固态硬盘提升大的存储盘的性能]]></title>
    <link href="klauscc.github.io/lvm-cache%20config:%20improve%20hdd%20performance%20via%20a%20ssd.html"/>
    <updated>2017-11-03T01:03:56-07:00</updated>
    <id>klauscc.github.io/lvm-cache%20config:%20improve%20hdd%20performance%20via%20a%20ssd.html</id>
    <content type="html"><![CDATA[
<p>在深度学习任务中，经常会频繁读取大量文件(如图片等)，这些大的数据集往往保存在机械硬盘上，机械硬盘上小文件读取速度缓慢，往往GPU需要等待硬盘IO而降低训练效率。<strong>本文讲解了怎么通过固态硬盘作cache提升系统IO性能</strong></p>

<ul>
<li>
<a href="#toc_0">1.什么是LVM</a>
</li>
<li>
<a href="#toc_1">2. LVM-Cache</a>
</li>
<li>
<a href="#toc_2">3. LVM + LVM-Cache 实战</a>
<ul>
<li>
<a href="#toc_3">3.1 创建PV, VG, LV</a>
</li>
<li>
<a href="#toc_4">3.2 创建chache pool lv</a>
</li>
<li>
<a href="#toc_5">3.3 缓存slow lv</a>
</li>
<li>
<a href="#toc_6">3.4 移除cache</a>
</li>
</ul>
</li>
</ul>


<span id="more"></span><!-- more -->

<h2 id="toc_0">1.什么是LVM</h2>

<p>LVM(logical Volume Manager) 是Linux环境下对磁盘分区进行管理的一种机制，他可以方便的调整各个逻辑分区的大小。</p>

<p>LVM主要有三个组成部分: </p>

<ol>
<li><strong>PV</strong>(Physical Volume),物理存储设备，一般是磁盘，如/dev/sda</li>
<li><strong>VG</strong>(Volume Group)，一个VG可以由多个PV组成，可以在VG上建立多个LV</li>
<li><strong>LV</strong>(logical Volume)类似于非LVM系统的磁盘分区，LV建立在VG之上，可以在LV之上建立文件系统(如mount 到 /home)</li>
</ol>

<h2 id="toc_1">2. LVM-Cache</h2>

<p><strong>LVM-Cache</strong> 用一个小的但是快的LV(fast-lv, <strong>cache pool LV</strong>)来提高大的但是慢的LV(slow-lv, origin LV)的性能。<br/>
它通过缓存slow-lv上被频繁访问的blocks到fast-lv上,应用再次访问这些blocks的时候就可以直接从fast-lv上面访问，从而大大提高了io性能。<br/>
<strong>cache pool LV</strong>被划分为两个lv: <code>cache data LV</code>, <code>cache metadata LV</code>。<code>cache data LV</code>是保存被缓存的slow-lv的blocks来提高速度, <code>cache metadata LV</code>用来保存保存blocks的相关元数据（cache了哪些block,这些block保存在了哪等)。 这些相关的LV都必须属于同一个VG.</p>

<h2 id="toc_2">3. LVM + LVM-Cache 实战</h2>

<p>现在有两个4T机械硬盘(slow):/dev/sda, /dev/sdb. 一个高速250G固态硬盘(fast): /dev/sdc. 下面介绍怎么通过 sdc 来cache 两个slow的机械硬盘</p>

<h3 id="toc_3">3.1 创建PV, VG, LV</h3>

<pre><code>#创建pv
pvcreate /dev/sda /dev/sdb /dev/sdc
#创建vg
vgcreate my_vg /dev/sda/ /dev/sdb 
#创建slow_lv
lvcreate -n slow_lv -l 100%FREE my_vg
#创建fast_lv
lvcreate -n cache_data -L 230G my_vg /dev/sdc
lvcreate -n cache_meta -L 250M my_vg /dev/sdc
</code></pre>

<h3 id="toc_4">3.2 创建chache pool lv</h3>

<pre><code>lvconvert --type cache-pool --poolmetadata my_vg/cache_meta my_vg/cache_data
</code></pre>

<h3 id="toc_5">3.3 缓存slow lv</h3>

<pre><code>lvconvert --type cache --cachepool my_vg/cache_data my_vg/slow_lv
</code></pre>

<p>至此slow_lv 被 cache_data缓存了<br/>
通过<code>lvs -a my_vg</code>查看my_vg状况</p>

<h3 id="toc_6">3.4 移除cache</h3>

<pre><code>lvremove my_vg/cache_data
lvconvert --uncache my_vg/slow_lv
</code></pre>

<p>ref: <a href="http://man7.org/linux/man-pages/man7/lvmcache.7.html">http://man7.org/linux/man-pages/man7/lvmcache.7.html</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[阅读笔记] Recursive Recurrent Nets with Attention Modeling for OCR in the Wild]]></title>
    <link href="klauscc.github.io/reading-note_Recursive-Recurrent-Nets-with-Attention-Modeling-for-OCR-in-the-Wild.html"/>
    <updated>2017-09-23T22:33:56-07:00</updated>
    <id>klauscc.github.io/reading-note_Recursive-Recurrent-Nets-with-Attention-Modeling-for-OCR-in-the-Wild.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1. 简介</a>
</li>
<li>
<a href="#toc_1">2. 方法介绍</a>
<ul>
<li>
<a href="#toc_2">2.1 Recursive Convolution</a>
</li>
<li>
<a href="#toc_3">2.2 RNNs for character-level language modeling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">3. 总结</a>
</li>
</ul>


<h2 id="toc_0">1. 简介</h2>

<p>论文提出了 \(R^2AM\) 模型用来在自然图片中的OCR。<br/>
主要有三个贡献:</p>

<pre><code>i. 用recursive cnn 来用同样多的参数却增加了网络的层数，即增加了特征提取能力。
ii. 用RNN来model character-level language。
iii. 用 soft-attention来选择更好的特征组合，并且可以end-to-end的backpropagation训练。
</code></pre>

<span id="more"></span><!-- more -->

<h2 id="toc_1">2. 方法介绍</h2>

<p>论文网络结构如图所示，先使用 Recursive CNN + 2 Fully connected Layer 提取\(D-dimension\)的特征 \(\daleth\)。再将\(\daleth\)输入RNN-Attention Model学习Character-Level model。<br/>
<img src="media/15062312362834/15062325813128.jpg" alt=""/></p>

<h3 id="toc_2">2.1 Recursive Convolution</h3>

<p>Recursive Convolution 简单来讲就是多个Convolution 层共享同样的权重 \(W,b\)。</p>

<p>对于某一层Convolution \(L_i\), 它的filter 个数为\(f_i\), kernel为\(k*k\), 它的前一层filter个数为\(f_{i-1}\), 则\(L_i\)层权重\(W\)的维度为 \(k*k*f_{i-1}*f_i\)。 也就是说，为了多个Convolution共享权重\(W\), 需要 \(f_{i-1} = f_i\)。但是cnn中一般要求filter数量逐渐增加当输入宽高逐渐减小，用Recursive Convolution就没办法增加filter数量。<br/>
为了解决这个问题，论文对于一个Recursive Convolution Block使用了两个权重\(W_{untied}\)和\(W_{tied}\)。简单来讲，就是先用一个Convolution层增加filter数量，然后后面t-1层都保持这个filter数量。</p>

<p><img src="media/15062312362834/15062325058704.jpg" alt=""/></p>

<h3 id="toc_3">2.2 RNNs for character-level language modeling</h3>

<p>论文中提出的模型如图所示。\( y = \{y_1, y_2,..., y_N\}, y_t \in \mathbb{R}^K\)。 \(K\)是总共character的个数,(&lt;sow&gt;, &lt;eow&gt; 也算)。 首先通过一个hidden RNN 学习 character-level language model, 再使用一个RNN通过image feature和character-level language model学习图片与输出间的关系。</p>

<p><img src="media/15062312362834/15062330475969.jpg" alt=""/></p>

<h2 id="toc_4">3. 总结</h2>

<p>这篇论文的主要贡献还是使用RNN来学习字符之间关系的模型。论文中比较了不同的RNN架构对结果的影响，并提出了\(RNN_{Atten}\)模型取得了state-of-the-art的结果。\(RNN_{Atten}\)的结构还是很有启发性和值得借鉴的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 配置静态ipv6地址]]></title>
    <link href="klauscc.github.io/config-static-ipv6-address-on-ubuntu.html"/>
    <updated>2017-09-05T20:32:56-07:00</updated>
    <id>klauscc.github.io/config-static-ipv6-address-on-ubuntu.html</id>
    <content type="html"><![CDATA[
<p>在网上搜了很多ipv6配置教程，然而重启以后并不会生效。解决办法:</p>

<p><code>vim etc/sysctl.d/10-ipv6-privacy.conf</code><br/>
将内容改为:</p>

<pre><code>net.ipv6.conf.all.use_tempaddr = 0
net.ipv6.conf.default.use_tempaddr = 0
</code></pre>

<span id="more"></span><!-- more -->

<p>这是因为IPV6 privacy extension默认被Ubuntu启用，这会导致系统会不断更改ipv6地址防止google/facebook追踪， 所以关掉以后静态ipv6配置才会生效。关闭方法见上面。</p>

<h3 id="toc_0">配置</h3>

<h4 id="toc_1">永久生效</h4>

<p><code>sudo vim /etc/network/interfaces</code></p>

<pre><code>iface eth0 inet6 static
        address primary_ipv6_address
        netmask 64
        gateway ipv6_gateway
        autoconf 0
        dns-nameservers 2001:4860:4860::8844 2001:4860:4860::8888 209.244.0.3
</code></pre>

<h4 id="toc_2">临时生效</h4>

<pre><code>sudo ip -6 addr add public_ipv6_address/64 dev eth0
sudo ip -6 route add default via public_ipv6_gateway dev eth0
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Recurrent Neural Networks前馈反馈推导]]></title>
    <link href="klauscc.github.io/Recurrent%20Neural%20Networks%E5%89%8D%E9%A6%88%E5%8F%8D%E9%A6%88%E6%8E%A8%E5%AF%BC.html"/>
    <updated>2017-05-16T03:06:16-07:00</updated>
    <id>klauscc.github.io/Recurrent%20Neural%20Networks%E5%89%8D%E9%A6%88%E5%8F%8D%E9%A6%88%E6%8E%A8%E5%AF%BC.html</id>
    <content type="html"><![CDATA[
<p>本文推导了RNN的前馈和反馈过程公式。</p>

<ul>
<li>
<a href="#toc_0">1. Introduction</a>
</li>
<li>
<a href="#toc_1">2. Forward Pass</a>
</li>
<li>
<a href="#toc_2">3. Backward Pass</a>
</li>
</ul>


<span id="more"></span><!-- more -->

<h2 id="toc_0">1. Introduction</h2>

<p><img src="media/14949291764036/14949296062380.jpg" alt=""/></p>

<p><img src="media/14949291764036/14949295667284.jpg" alt=""/></p>

<p>输入一个时间序列X(I,T),输出一个时间序列Y(K,T), 时间t+1时刻的神经元会利用到t时刻的输出值。</p>

<h2 id="toc_1">2. Forward Pass</h2>

<p>前馈从输入层到输出层<br/>
考虑一个RNN:<br/>
    1. 输入层为I个神经元，序列长度为T。则输入x维度为(I,T),维度不考虑batch_size<br/>
    2. 隐层(hidden layer)有H个神经元<br/>
    3. 输出层有K个神经元.输出维度为(K,T)</p>

<p>输入为\(x:\{x^1,x^2,...,x^T\}\),其中\(x^t = \{x_1^t,x_2^t,...,x_I^t\}\)。则\(x_i^t\)为时间t第i个神经元的输入。<br/>
对于第L层，\(a_i^t\)为这一层的输入，\(b_i^t\)为这一层的输出。这一层的权重\(W = \{w_{ij}\}_{h\times k}\),h为L-1层的神经元个数，k为L层的神经元个数。\(w_{ij}\)为第L-1层第i个神经元与第L层第j个神经元的连接权重。</p>

<p>每一层的输入输出如下。\(\theta(x)为激活函数，rnn中一般为sigmoid。\theta&#39;(x) = \theta(x)(1-\theta(x))\)<br/>
Hidden Layer:<br/>
    1. Input(x): \(x: \{x_i^t\}\)<br/>
    2. Output(\(b_h^t\)):<br/>
    <center>\(a_h^t = \sum_{i=1}^{I}w_{ih}x_i^t + \sum_{h&#39;=1}^{H}w_{h&#39;h}b_{h&#39;}^{t-1}\)<br/>
    \(b_h^t = \theta(a_h^t)\)</center><br/>
Output Layer:<br/>
    1. Input(\(b_h^t\))<br/>
    2. Output:<br/>
<center><br/>
    \(a_k^t = \sum_{h=1}^Hw_{hk}b_h^t\)<br/>
    \(b_k^t = \theta(a_k^t)\)</center></p>

<p>写成矩阵形式为<br/>
    \(A_H^t = W_1^TX^t+ W_2^TA_H^{t-1}\), shape: (H,1)<br/><br/>
    \(B_H^t = \theta(A_H^t)\), shape:(H,1)， \(\theta(V)\)表示对矩阵\(V\)的每个元素\(v_{ij}\)用\(\theta(v_{ij})\)进行计算<br/>
    \(A_K^t = W_3^TB_H^t\), shape: (K,1)<br/>
    \(B_K^t = \theta(A_K^t)\), shape:(K,1)</p>

<p>注意到求\(A_H^t\)需要依赖于\(A_H^{t-1}\)，所以隐层在时间维度T是无法用矩阵并行运算而需要从t=0到t=T-1依次计算。但是反向求梯度的时候可以将时间维度T并行起来。</p>

<h2 id="toc_2">3. Backward Pass</h2>

<p>反向传播从输出层反向传梯度到输入层</p>

<p><strong>输出层</strong>: with respect to Layer Output(\(b_k^t\)), Parameter(\(w_{hk}\)), Input(\(b_h^t \))<br/>
\(\delta(b_k^t) = \frac{\partial L}{\partial b_k^t}\)<br/>
\(\delta(a_k^t) = \theta&#39;(a_k^t)\delta(b_k^t)\)<br/>
\(\delta(w_{hk}) = \frac{\partial L}{\partial w_{hk}} = \sum_{t=1}^{T}\frac{\partial L}{\partial a_k^t}* \frac{\partial a_k^t}{w_{hk}} = \sum_{t=1}^{T} \delta(a_k^t)b_h^t \)<br/>
<strong>隐层</strong>: with respect to Layer Output(\(b_h^t\)),parameter(\(w_{ih}, w_(h&#39;h)\)), input(\(x_i^t\))<br/>
\(\delta(b_h^t) = \frac{\partial L}{\partial b_h^t} = \sum_{k=1}^{K}\frac{\partial L}{\partial a_k^t}*\frac{\partial a_k^t}{b_h^t} + \sum_{h&#39;=1}^{H}\frac{\partial L}{\partial b_{h&#39;}^{t+1 }}*\frac{\partial b_{h&#39;}^{t+1}}{\partial b_h^{t}} \)<br/>
 \( = \sum_{k=1}^{K}\delta(a_k^t)w_{hk} + \sum_{h&#39;=1}^{H}\delta(b_{h&#39;}^{t+1})w_{hh&#39;}\)<br/>
 \(\delta(a_h^t) = \frac{\partial L}{a_h^t} = \frac{\partial L}{b_h^t} * \frac{\partial b_h^t}{a_h^t} = \delta(b_h^t) \theta&#39;(a_h^t)\)</p>

<p>\(\delta(w_{ih}) = \frac{\partial L}{\partial w_{ih}} = \frac{\partial L}{\partial a_h^t} * \frac{\partial a_h^t}{\partial w_{ih}} = \delta(a_h^t) * x_i^t\)</p>

<p>\(\delta(w_{h&#39;h}) = \frac{\partial L}{\partial w_{h&#39;h}} = \frac{\partial L}{\partial a_h^t}* \frac{\partial a_h^t}{\partial w_{h&#39;h}} = \sum_{t=1}^{T}\delta(a_h^t)b_{h&#39;}^{t-1}\)</p>

<p>写成矩阵的形式:<br/>
计Loss函数关于某个变量V(input, output, parameter)的梯度\(Grad(V) = \Delta(V)\)</p>

<p>输出层:输入\(B_H:(H,T)\) 输出\(B_K:(K,T)\) 参数\(W_3:(H,K)\) <br/>
\(\Delta(B_K) = \frac{\partial L}{\partial B_K} \) shape(K,T)<br/>
\(\Delta(A_K) = \Delta(B_K) \odot \theta&#39;(A_K)\) shape(K,T)， \(\odot\)表示element-wise 乘法，即两个size相同的矩阵的每个对应元素相互乘起来。<br/>
\(\Delta(W_3) = B_H*\Delta^T(A_K)\) shape(H,K)<br/>
隐层: 输入\(X:(I,T)\) 输出\(B_H:(H,T)\) 参数\(W_1:(I,H),W_2:(H,H) \)<br/>
\(\Delta(B_H) = W_3*\Delta(A_K) + W_2*(\Delta(B_H) &lt;&lt; 1, axis=2)\) shape(H,T). \(\Delta(B_H) &lt;&lt; 1, axis=2\) 表示矩阵\(\Delta(B_H)\)在时间T的维度向左移位1次<br/>
\(\Delta(A_H) = \Delta(B_H) \odot \theta&#39;(B_H)\) ,shape(H,T)<br/>
\(\Delta(W_1) = X * \Delta(A_H) \)<br/>
\(\Delta(W_2) = (B_H &gt;&gt; 1, axis=2)*\Delta^T(A_H)\), shape(H,H)</p>

<p>对于第L层，求出其输出梯度，从而求出其输入梯度。利用输出输入梯度，可以求得第L层参数梯度，从而更新第L层参数。<br/>
而第L-1层的输出梯度为第L层的输入梯度。<br/>
依次反向传播更新每层参数。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[centos 安装shadowsocks,proxychains,genpac科学上网]]></title>
    <link href="klauscc.github.io/shadowsocks-proxychains-genpac-on-centos.html"/>
    <updated>2017-02-12T22:28:05-08:00</updated>
    <id>klauscc.github.io/shadowsocks-proxychains-genpac-on-centos.html</id>
    <content type="html"><![CDATA[
<p>本文说明的是如何让centos系统能够翻墙，而不是作为ss服务器。<br/>
shadowsocks 翻墙客户端<br/>
proxychains 让命令行可以通过ss代理<br/>
genpac生成proxy.pac文件以达到局部代理效果</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">1. 安装 shadowsocks</a>
</li>
<li>
<a href="#toc_1">2. 安装 proxychains-ng,让命令行可以翻墙</a>
</li>
<li>
<a href="#toc_2">3. 安装genpac, 让浏览器局部代理</a>
</li>
</ul>


<h3 id="toc_0">1. 安装 shadowsocks</h3>

<p>默认有root权限，否则在命令前加<code>sudo</code></p>

<p>如果已经安装了pip则</p>

<pre><code>pip install shadowsocks
</code></pre>

<p>pip 安装</p>

<pre><code>wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
</code></pre>

<p>安装完毕以后创建配置文件 <code>/etc/shadowsocks.json</code></p>

<pre><code>{
&quot;server&quot;:&quot;your-ssserver-ip-address&quot;,
&quot;server_port&quot;:your-port,
&quot;local_address&quot;: &quot;127.0.0.1&quot;,
&quot;local_port&quot;:1080,
&quot;password&quot;:&quot;your-password&quot;,
&quot;timeout&quot;:600,
&quot;method&quot;:&quot;aes-256-cfb&quot;
}
</code></pre>

<p>运行sslocal:</p>

<pre><code>sslocal -c /etc/shadowsocks.json -d start
</code></pre>

<p>让开机运行:</p>

<pre><code>crontab -e
</code></pre>

<p>然后再最后添加一行:</p>

<pre><code>@reboot /usr/bin/sslocal -c /etc/shadowsocks.json -d start
</code></pre>

<h3 id="toc_1">2. 安装 proxychains-ng,让命令行可以翻墙</h3>

<p>proxychains-ng github地址:<a href="https://github.com/rofl0r/proxychains-ng">https://github.com/rofl0r/proxychains-ng</a></p>

<pre><code>git clone https://github.com/rofl0r/proxychains-ng
cd proxychains-ng
./configure --prefix=/usr --sysconfdir=/etc
make 
make install
sudo make install-config
</code></pre>

<p>安装以后修改配置:</p>

<pre><code>vim /etc/proxychains.conf
</code></pre>

<p>找到<code>[ProxyList]</code>,修改为</p>

<pre><code>[ProxyList]
 # add proxy here ...
 # meanwile
 # defaults set to &quot;tor&quot;
 socks5  127.0.0.1 1080
 #socks4     127.0.0.1 9050
</code></pre>

<p>配置完以后就可以在命令行翻墙了,测试:</p>

<pre><code>proxychains4 wget www.google.com
</code></pre>

<h3 id="toc_2">3. 安装genpac, 让浏览器局部代理</h3>

<p>安装:</p>

<pre><code>pip install genpac
</code></pre>

<p>安装完以后，我们可以生成proxy.pac文件并保存在一个目录下,比如<code>/home/config/shadowsocks</code></p>

<pre><code>mkdir -p /home/config/shadowsocks
cd /home/config/shadowsocks
genpac --proxy=&quot;SOCKS5 127.0.0.1:1080&quot; --gfwlist-proxy=&quot;SOCKS5 127.0.0.1:1080&quot; -o autoproxy.pac --gfwlist-url=&quot;https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt&quot;
</code></pre>

<p>然后会生成一个<code>autoproxy.pac</code>文件</p>

<p>之后就是配置浏览器代理，以firefox为例,<code>Preferences&gt;Advanced&gt;Network&gt;Settings</code><br/>
<img src="media/14869672857615/14869685627762.jpg" alt=""/></p>

<p>配置好以后，在浏览器中打开<code>www.google.com</code>测试是否成功。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习、caffe简要入门指南]]></title>
    <link href="klauscc.github.io/caffe-simple-beginner-guide.html"/>
    <updated>2017-01-14T22:46:58-08:00</updated>
    <id>klauscc.github.io/caffe-simple-beginner-guide.html</id>
    <content type="html"><![CDATA[
<p>近年来，深度学习是机器学习的一大潮流。它在计算机视觉CV，自然语言处理NLP等领域取得了极大的成就。本文以深度学习框架Caffe作为工具简要讲解深度学习入门。</p>

<p><strong>对深度学习和CNN比较了解的可以直接跳至第四节，对caffe比较了解的可以直接跳过本教程。</strong></p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">1. 深度学习概要</a>
</li>
<li>
<a href="#toc_1">2. 神经网络</a>
</li>
<li>
<a href="#toc_2">3. 卷积神经网络(Convolutional Neural Networks, CNNS or ConvNets)</a>
</li>
<li>
<a href="#toc_3">4. Caffe 实战</a>
<ul>
<li>
<a href="#toc_4">4.1 数据准备</a>
</li>
<li>
<a href="#toc_5">4.2 定义模型</a>
</li>
<li>
<a href="#toc_6">4.3 定义solver</a>
</li>
<li>
<a href="#toc_7">4.4 训练模型</a>
<ul>
<li>
<a href="#toc_8">4.4.1 训练</a>
</li>
<li>
<a href="#toc_9">4.4.2 可视化</a>
</li>
<li>
<a href="#toc_10">4.4.3 测试</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">4.5 迁移学习</a>
<ul>
<li>
<a href="#toc_12">4.5.1 使用迁移学习解决猫狗分类</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_13">结论</a>
</li>
</ul>


<h3 id="toc_0">1. 深度学习概要</h3>

<p>深度学习模型是从<a href="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>演变而来，主流的两个模型是CNN(Convolutional Neural Network,卷积神经网络)和RNN(Recurrent Neural Network,循环神经网络)，分别对应CV领域和NLP领域。CNN的经典实例有Alexnet, Googlenet, VGG, Resnet等模型，RNN最经典的是LSTM。经过这些年的发展，在这两个领域新推出的模型几乎都是CNN和RNN的变种和组合。</p>

<p>深度学习与传统机器学习最大的区别是特征由网络自动学习，而不需要手工设计特征。<br/>
<center ><img src="media/14844628181225/14850557732034.jpg" alt=""/></center ></p>

<p>作为入门，本文以caffe为工具，以<a href="https://www.kaggle.com/c/dogs-vs-cats">猫狗图片分类问题</a>， 使用Alexnet模型来简要介绍深度学习。</p>

<h3 id="toc_1">2. 神经网络</h3>

<p>神经元是大脑的基本组成成分，神经网络正是由生物的神经元激发的灵感，它由一个个神经元连接而成，每个神经元有一个激活函数(Sigmoid, ReLu,等)。下图是一个2隐层的前馈神经网络。<br/>
<center ><img src="media/14844628181225/14850561002582.jpg" alt=""/> </center></p>

<p>每两个神经元之间的连接有一个权值系数，而深度学习学习的目标就是找到一组这样的参数使得网络输出值与实际输出值之间的差别最小，这个差别使用Loss Function(损失函数，or 代价函数cost function)来衡量。一个最简单的损失函数是<strong>均方差损失函数</strong>(Squared-error cost function):<br/>
<center> \(J(W,b;x,y) = \frac{1}{2}||h_{W,b}(x) - y||^2\) </center><br/>
式中W,b是网络的权值，也是我们需要求的系数，\(h_{W,b}(x)\)是网络在参数W,b下的x的输出。</p>

<p>学习的目标是<strong>最小化Loss函数</strong>，即是首先条件下求极值的问题。对于两三个参数的极值问题可以使用拉格朗日乘数法求极值，这就是<strong>符号微分</strong>。但是对于我们的网络，可能需要优化的参数有几百万个，符号微分方法不可行或者代价昂贵，在所有机器学习算法中使用的是<strong>自动化微分</strong>方法来求解。</p>

<p>自动化微分的方法基本思想如下:<br/>
    1. 训练开始时随机初始化参数。<br/>
    2. 计算输入x在当前参数下的输出\(h_{W,b}(x)\)(Forward)<br/>
    3. 计算Loss函数J的梯度，并使用梯度更新参数W，b(Backward)，参数更新方式有SGD,AdaGrad等方法。<br/>
    4. 2和3是一个iteration(迭代)，如此进行多个迭代直至满足某一条件为止。</p>

<p><strong>每个iteration选取训练集的一个batch的数据进行训练，一个epoch是指将训练集遍历完所需要的iteration的个数。</strong></p>

<p><strong>具体学习过程参考UFLDL教程</strong>:<a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">Multi-Layer Neural Network<br/>
</a></p>

<p>这种方法非常适合计算机矢量化实现，而且目前有很多优化的很好的张量运算库如mkl, cudnn等。</p>

<h3 id="toc_2">3. 卷积神经网络(Convolutional Neural Networks, CNNS or ConvNets)</h3>

<p>CNN是多层网络，由卷积层(Convolution),池化层(Pooling), Batch Normalization(BN,标准化，将输出变为方差0，均值1的高斯分布，防止“梯度弥散”。关于梯度弥散，一个简单的例子：0.9<sup>{30}\approx</sup> 0.04，BN将经过多层变得很小的梯度的scale变大)，全连接层(Full Connection, fc),Dropout(随机失活某些神经元，避免过拟合)等组成，每层是其中的一种。第2节中讲到的前馈神经网络是一个由3层全连接层构成的网络。CNN的训练过程和第2节训练方法一致。</p>

<p><strong>具体讲解参考UFLDL教程<a href="http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/">Multi-Layer Neural Network<br/>
</a></strong></p>

<p>卷积层效果图。相当于将一个固定大小的kernel(卷积核)以stride的步长对图片进行遍历。需要注意的有kernel_size, stride, pad几个参数。kernel_size表示卷积核的大小;stride表示步长，即遍历的时候每次前进的大小;pad表示对原始图片两边(水平方向和竖直方向，pad_h和pad_w)进行补零以获得期望的输出尺寸。下图是以3x3的kernel, 1的stride, 0的pad对5x5(h_i x w_i)的图片进行convolution,输出大小为3x3(h_o x w_o)。<strong>计算公式 h_o = (h_i + 2 * pad_h - kernel_h) / stride_h + 1, w_o 类似</strong><br/>
<img src="media/14844628181225/14855025028447.gif" alt=""/></p>

<p><center><img src="media/14844628181225/14850653780486.jpg" alt=""/></center></p>

<p>本文中即将使用的Alexnet结构如下:<br/>
<center><br/>
<img src="media/14844628181225/alxenet.jpg" alt="alxenet" style="width:5024px;"/><br/>
</center></p>

<h3 id="toc_3">4. Caffe 实战</h3>

<p>caffe提供c++, python, matlab和shell接口。python 和matlab接口提供类似功能，一般在predict,feature extraction的时候才会用到，用于与其他模块结合。但是一般的训练测试过程只需要用到shell接口。</p>

<p>在Caffe中训练模型有下面4步:</p>

<pre><code>1. 数据准备。将数据处理为Caffe需要的形式。
2. 定义Model。在Caffe中定义模型架构和参数使用`.prototxt`后缀名的文件
3. 定义Solver。Solver是Caffe训练模型的入口，里面定义一些超参数如:学习率，最大Iteration数，模型参数文件保存路径等。
4. 训练模型。在命令行执行caffe的命令即可训练模型。
</code></pre>

<p>示例代码路径: <a href="https://github.com/klauscc/dogOrCat">https://github.com/klauscc/dogOrCat</a></p>

<p>上面四步只会用到shell接口。</p>

<h4 id="toc_4">4.1 数据准备</h4>

<p>从 <a href="https://www.kaggle.com/c/dogs-vs-cats/data">Dogs vs. Cats</a>下载数据后解压</p>

<pre><code>unzip ~/train.zip
unzip ~/test1.zip
rm ~/deeplearning-cats-dogs-tutorial/input/*.zip
</code></pre>

<p>Caffe的输入可以使用lmdb格式的数据，也可以使用原始图片的label文件(每一行是一张图片的路径和类别)。</p>

<p>创建lmdb数据库方式可以参考caffe源码中<code>./examples/imagenet/create_imagenet.sh</code>。如果你的训练数据不是原始图片(比如进行过预处理)，就必须使用lmdb的方式载入数据了,而创建方式就需要使用python或者c++接口了，需要的时候google <code>caffe python lmdb</code>就有大把的教程了。</p>

<p>本文使用的是原始图片进行训练，caffe需要的文件内容如下:</p>

<pre><code>./data/train/cat.10607.jpg 0
./data/train/cat.7659.jpg 0
./data/train/dog.3057.jpg 1
./data/train/dog.5977.jpg 1
./data/train/cat.9318.jpg 0
./data/train/dog.625.jpg 1
./data/train/cat.12339.jpg 0
./data/train/cat.4378.jpg 0
./data/train/cat.9347.jpg 0
./data/train/dog.2431.jpg 1
./data/train/cat.12228.jpg 0
./data/train/dog.10660.jpg 1
</code></pre>

<p>生成这个文件<a href="https://github.com/klauscc/dogOrCat/blob/master/splitDataset.py">代码</a>:</p>

<pre><code>import numpy as np
import glob
import os


#get all the images
database_dir=&quot;./data&quot;
image_data = [img for img in glob.glob(database_dir+&quot;/train/*&quot;)]

f_tv = open(&#39;train_val.txt&#39;,&#39;w&#39;)
f_test = open(&#39;test.txt&#39;,&#39;w&#39;)

#division the dataset into train and test set
for in_idx, img_path in enumerate(image_data):
    if &#39;dog&#39; in img_path:
        label=1
    else:
        label=0

    if in_idx % 5 ==0:
        f_test.write(&quot;%s %d\n&quot; %(img_path, label))
    else:
        f_tv.write(&quot;%s %d\n&quot; %(img_path, label))

f_tv.close()
f_test.close()
</code></pre>

<h4 id="toc_5">4.2 定义模型</h4>

<p>caffe 中网络模型文件使用<code>prototxt</code>后缀保存。<a href="https://github.com/klauscc/dogOrCat/blob/master/alexnet/train_val.prototxt">模型文件</a></p>

<p>其基本结构如下</p>

<pre><code>name: &#39;alexnet&#39;

layer{
 name: &quot;data&quot;
  type: &quot;ImageData&quot;
  top: &quot;data&quot;
  top: &quot;label&quot;
  include {
    phase: TRAIN
  }
  transform_param {
  }
}

layer {
    bottom: &quot;data&quot;
    top: &quot;conv1&quot;
    ...
}

layer {

}
...
</code></pre>

<p>层间关系通过 <code>top</code>和<code>bottom</code>指定，这样一个网络结构就唯一确定下来了(回顾第三节alexnet结构图)。 这里挑选其中几层讲解,参数意义看注释</p>

<pre><code>layer { #输入层
  name: &quot;data&quot;  
  type: &quot;ImageData&quot; #类型，这种类型使用4.1生成的label文件
  top: &quot;data&quot;  #输出1
  top: &quot;label&quot; #输出2
  include {
    phase: TRAIN #阶段，训练阶段的输入层;测试阶段的输入层:TEST。在solver中可以指定多少个iteration TEST一次
  }
  transform_param {
    mirror: true  # data augmentation,将图片的水平翻转也加入数据集增加训练数据
    crop_size: 227 #将图片crop到多大227x227
    mean_value: 104 #归一化，rgb的mean value
    mean_value: 117
    mean_value: 123
  }
  image_data_param {
    source: &quot;train_val.txt&quot; #数据来源
    batch_size: 256 #batch 大小，一个iteration训练多少数据
    new_height: 256
    new_width: 256
    shuffle: true  # 是否打乱训练数据
  }
}
</code></pre>

<p>这个输入层输出的数据尺寸为256x3x227x227(nchw. n:batchsize;c:channel;h:height;w:width. tensorflow中默认格式是nhwc)</p>

<p>卷积层</p>

<pre><code>layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot; 
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    pad: 0
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0
    }
  }
}
</code></pre>

<p>这个卷积层输入256x3x227x227(bottom: &quot;data&quot;), kernel_size, stride, pad,输出计算方法请看第三节,输出尺寸为 256x96x55x55. weight_filter 和 bias_filter是convolution参数的<strong>初始化</strong>方法。</p>

<p><strong>其他各层以及每层参数意义可以参考<a href="http://caffe.berkeleyvision.org/tutorial/layers.html">caffe官方文档</a></strong></p>

<h4 id="toc_6">4.3 定义solver</h4>

<p>solver文件是caffe训练的入口文件，里面定义各种超参数。示例:</p>

<pre><code>net: &quot;alexnet/train_val.prototxt&quot;  #网络模型路径
test_iter: 200     #第一次测试的iteration
test_interval: 200 #每隔多少个iteration 对模型测试一次
test_initialization: false #是否开始训练时测试一次
base_lr: 0.001 #基础学习率(learning rate, lr)
lr_policy: &quot;step&quot; #lr 下降策略,step. 每隔stepsize个iteration降低lr, lr&#39; = lr * gamma
gamma: 0.1
stepsize: 2000 # 每隔2000个iteration调整一次学习率lr
display: 20  # 每隔20个iteration 打印一次输出(loss, accuracy等)
max_iter: 450000 # 到达max_iter则停止训练
momentum: 0.9
weight_decay: 0.0005 #防止过拟合, 给loss函数加的正则项(regulation, L1,L2)前的系数
snapshot: 5000 #每隔多少个iteration保存一次模型
snapshot_prefix: &quot;/data/tmp/klaus/dogVsCat/alexnet/dogvscat_alexnet_train&quot; #模型参数文件保存路径
solver_mode: GPU #使用GPU进行训练
</code></pre>

<p><strong>更多详细参数参考 <a href="http://caffe.berkeleyvision.org/tutorial/solver.html">caffe官方文档</a></strong></p>

<h4 id="toc_7">4.4 训练模型</h4>

<h5 id="toc_8">4.4.1 训练</h5>

<p>在定义了模型结构(train_val.prototxt)和训练超参数(solver.prototxt)以后，就可以进行训练了。</p>

<pre><code>caffe train -solver ./alexnet/solver.prototxt 2&gt;&amp;1| tee dogVsCat.log
</code></pre>

<p>其中<code>2&gt;&amp;1| tee dogVsCat.log</code> 这一行是将控制台输出保存到文件中。<code>2&gt;&amp;1</code> 是将stderr重定向到stdout中</p>

<h5 id="toc_9">4.4.2 可视化</h5>

<p>在训练完以后，可以使用<code>dogVsCat.log</code>文件画出学习曲线</p>

<pre><code>python plot_learning_curve.py ./dogVsCat.log ./dogcat_learning_curve.png
</code></pre>

<p><img src="media/14844628181225/14855045855566.jpg" alt=""/></p>

<h5 id="toc_10">4.4.3 测试</h5>

<p>测试模型</p>

<pre><code>caffe test -model ./alexnet/train_val.prototxt -weights /data/tmp/klaus/dogVsCat/alexnet/dogvscat_alexnet_train_iter_10000.caffemodel  2&gt;&amp;1| tee dogVsCat_test.log
</code></pre>

<p>这里测试的数据是<code>train_val.prototxt</code>中<code>phase:TEST</code>对应的输入层的数据， -weights是网络权重，是训练过程保存的模型参数文件</p>

<h4 id="toc_11">4.5 迁移学习</h4>

<p>深度学习需要大量数据和大量资源进行训练。而迁移学习是将一个在其他数据库上训练过的模型迁移到我们的数据库上，<strong>参数初始化为其他数据库上训练完的参数</strong>，这样可以在很短时间内达到很好的效果。</p>

<p>迁移学习在深度学习上一般有两种使用方式:</p>

<pre><code>1. 用已训练的模型作为feature extractor: 就是固定网络的前面的若干层，只调整最后的几个全连接层
2. fine-tune 已训练的模型: 在原有参数基础上继续训练所有层，这时候往往需要调低学习率(lr)
</code></pre>

<h5 id="toc_12">4.5.1 使用迁移学习解决猫狗分类</h5>

<p>imagenet是一个具有1280000张图片，1000种分类的巨大数据库。而Alexnet在在2012年Imagenet大赛上获得了冠军，也是让深度学习再次火起来的原因之一。caffe提供了在Imagenet数据库上训练的Alexnet的模型参数。</p>

<p>我们首先从 caffe 的 <a href="http://caffe.berkeleyvision.org/model_zoo.html">model zoo</a>下载alexnet模型. caffe 源码提供了相应脚本:</p>

<pre><code>cd $CAFFE_ROOT #change $CAFFE_ROOT with your caffe source code dir path or set the environment variable.
python scripts/download_model_binary.py models/bvlc_alexnet
</code></pre>

<p>这时候模型文件<code>bvlc_alexnet.caffemodel</code>下载到了<code>$CAFFE_ROOT/models/bvlc_alexnet/</code>目录下了，复制到我们的路径下，然后进行训练:</p>

<pre><code>caffe train -solver ./alexnet/solver.prototxt -weights ./alexnet/bvlc_alexnet.caffemodel  2&gt;&amp;1| tee dogVsCat.log
</code></pre>

<p><code>-weights ./alexnet/bvlc_alexnet.caffemodel</code>表示参数初始化为<code>./alexnet/bvlc_alexnet.caffemodel</code>的参数。</p>

<p>训练完后画出learning curve:<br/>
<img src="media/14844628181225/14855056406821.jpg" alt=""/></p>

<p>可以看到，相比于从头训练，模型的loss下降得更快而且最后效果更好。</p>

<h3 id="toc_13">结论</h3>

<p>关于使用python接口进行预测图片参考<a href="https://github.com/klauscc/dogOrCat/blob/master/make_predictions.py">make_predictions</a>,这里不做赘述。python接口往往是用来做application的，比如web_demo之类的。</p>

<p>本文对caffe使用方法进行简单但是全面的介绍，对深度学习有些了解的童鞋应该可以使用caffe来写自己的模型了。</p>

<p>对深度学习的进一步了解建议学习<a href="http://ufldl.stanford.edu/tutorial/supervised/ExerciseConvolutionalNeuralNetwork/">UFLDL教程</a> 和阅读<a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap">经典Paper</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[centos7.0安装配置，cuda,cudnn安装，anaconda安装，深度学习框架caffe,torch,theano,tensorflow安装]]></title>
    <link href="klauscc.github.io/centos-setup-deeplearning-environment.html"/>
    <updated>2016-12-06T20:57:59-08:00</updated>
    <id>klauscc.github.io/centos-setup-deeplearning-environment.html</id>
    <content type="html"><![CDATA[
<p>深度学习在linux上面会比windows上面方便很多，在windows上那叫个折腾。本文将会介绍centos7.0 的<br/>
安装，cuda和cudnn的安装，anaconda安装，以及各种深度学习框架的安装。</p>

<p>深度学习是计算密集型任务，不推荐在虚拟机中运行，装cuda和cudnn需要有nvidia显卡。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">1. 安装系统</a>
</li>
<li>
<a href="#toc_1">2. 配置网络</a>
<ul>
<li>
<a href="#toc_2">2.1 修改ip和网关</a>
</li>
<li>
<a href="#toc_3">2.2 配置DNS服务器</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">3. 配置安装ssh</a>
</li>
<li>
<a href="#toc_5">4. 安装cuda,cudnn</a>
<ul>
<li>
<a href="#toc_6">4.1 安装 cuda</a>
<ul>
<li>
<a href="#toc_7">4.1.1 关闭Nouveau驱动</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">4.1.2 安装cuda</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">4.2 安装cudnn</a>
</li>
<li>
<a href="#toc_10">5. 安装 caffe</a>
<ul>
<li>
<a href="#toc_11">5.1 安装依赖</a>
</li>
<li>
<a href="#toc_12">5.2 修改Makefile.config</a>
</li>
<li>
<a href="#toc_13">5.3 编译并测试</a>
</li>
</ul>
</li>
</ul>


<p><strong>本文默认在root下运行所有命令</strong></p>

<h3 id="toc_0">1. 安装系统</h3>

<pre><code>  建议不要使用最小安装，否则很多工具需要自己再次安装比较麻烦。
</code></pre>

<p>分区：自己分区，选择硬盘(比如sdb), 分区表如下<br/>
<img src="media/14810866793903/14810871998928.jpg" alt=""/></p>

<p>选择了需要安装的软件后不停下一步就可以安装完毕。</p>

<p>安装完成后，添加源 <code>yum install epel-release</code>,这个源包含了很多base源没有的软件包，非常实用</p>

<h3 id="toc_1">2. 配置网络</h3>

<p>centos 默认是没有联网的，如果系统没有安装图像界面如<code>gnome</code>或者<code>kde</code>等，需要在命令行配置如下</p>

<h4 id="toc_2">2.1 修改ip和网关</h4>

<pre><code>vim /etc/sysconfig/network-scripts/ifcfg-enp5s0 #enp5s0是网卡对应名称，使用`ifconfig`查看自己机器的网卡名称，然后修改对应文件
</code></pre>

<p>将<code>ONBOOT=no</code>改为<code>ONBOOT=yes</code>, 这样开机就会自动联网了</p>

<p>如果希望使用静态ip,配置如下(<strong>可选</strong>):</p>

<pre><code>#静态ip的好处是重启后ip不变，在路由器开启端口转发，ssh可以正常使用(否则ip变化会找不到服务器)。
BOOTPROTO=static     #静态获取
IPADDR=192.168.1.139 #指定ip地址
GATEWAY=192.168.1.1  # 指定网关
ONBOOT=yes #重启自动联网
</code></pre>

<p>配置好了在命令行运行<code>systemctl restart network.service</code>就会联网了。</p>

<h4 id="toc_3">2.2 配置DNS服务器</h4>

<p>编辑文件<code>/etc/resolv.conf</code> </p>

<pre><code>vim /etc/resolv.conf 
</code></pre>

<p>添加以下几行</p>

<pre><code>#这两个DNS服务器是交大的DNS服务器，可以自行更改最适合自己的DNS服务器
nameserver 202.120.2.101 #主DNS服务器
nameserver 202.120.2.100 #备用DNS服务器
</code></pre>

<p>修改完了以后在命令行输入<code>ping www.baidu.com</code>看是否能ping通，ping的通说明配置完成。</p>

<h3 id="toc_4">3. 配置安装ssh</h3>

<p>centos默认已经安装了OpenSSH,所以只需配置即可。配置文件为<code>/etc/ssh/sshd_config</code><br/>
修改下面几行</p>

<pre><code>Port 22 #22修改为自己想要的端口
PermitRootLogin no #no 改为yes, 是否允许root通过ssh登录。改为yes方便些
ClientAliveInterval 0 #去掉前面的&#39;#&#39;注释，并将0改为60.这个是让服务器每隔60s发送一个包给客户端，避免长时间不操作客户端会timeout连接关闭
</code></pre>

<p>修改好了再命令行运行<code>systemctl restart sshd.service</code> 重启sshd服务</p>

<p>如果ssh连接不上可能需要关闭selinux或防火墙才行:<br/>
<strong>关闭selinux</strong>: 修改 <code>/etc/sysconfig/selinux</code> 将里面<code>SELINUX=enforcing</code>改为 <code>SELINUX=disabled</code><br/>
<strong>关闭防火墙</strong>: 命令行运行<code>systemctl stop firewalld</code></p>

<h3 id="toc_5">4. 安装cuda,cudnn</h3>

<h4 id="toc_6">4.1 安装 cuda</h4>

<p>cuda是英伟达显卡驱动，可以去官网下载<a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>,可能需要注册一个账号</p>

<h5 id="toc_7">4.1.1 关闭Nouveau驱动</h5>

<p>Nouveau 是CentOs自带的驱动，要安装cuda需要先禁用掉这个驱动.<br/>
在命令行输入<code>lsmod | grep nouv</code>, 如果有输出表明这个驱动没用被禁用。</p>

<p>禁用方法:<br/>
修改或添加文件<code>/etc/modprobe.d/blacklist-nouveau.conf</code>:</p>

<pre><code>vim /etc/modprobe.d/blacklist-nouveau.conf
</code></pre>

<p>添加以下内容:</p>

<pre><code>blacklist nouveau
options nouveau modeset=0
</code></pre>

<p>添加完毕后在命令行运行以下命令</p>

<pre><code>sudo dracut --force
reboot
</code></pre>

<h4 id="toc_8">4.1.2 安装cuda</h4>

<p>重启后运行安装文件即可<code>bash cuda_8.0.44_linux-run</code><br/>
安装路径默认即可，安装完毕后需要添加环境变量，如果希望给所有用户安装则添加到<code>/etc/profile</code>,否则添加到<code>~/.bashrc</code></p>

<p>添加以下内容:</p>

<pre><code># /user/local/cuda是cuda安装路径，根据自己情况修改
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 
</code></pre>

<h3 id="toc_9">4.2 安装cudnn</h3>

<p>cudnn从官网上下下来包是编译后的包,只需要解压到对应路径并添加环境变量即可,建议解压到<code>/usr/local/cudnn</code>(原安装包解压后为cuda/*， <code>mv cuda /usr/local/cudnn</code>即可)</p>

<p>安装包下载地址<a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>,需要注册账号</p>

<p>解压到<code>/usr/local/cudnn</code>后只需要添加环境变量即可，为所有用户安装添加到<code>/etc/profile</code></p>

<pre><code>export C_INCLUDE_PATH=/usr/local/cudnn/include:$C_INCLUDE_PATH
export CPLUS_INCLUDE_PATH=/usr/local/cudnn/include:$CPLUS_INCLUDE_PATH
export LD_LIBRARY_PATH=/usr/local/cudnn/lib64:$LD_LIBRARY_PATH
</code></pre>

<pre><code>说明:
$C_INCLUDE_PATH和$CPLUS_INCLUDE_PATH分别是c和c++头文件存放路径，添加了以后程序就可以找到对应的头文件了，
$LD_LIBRARY_PATH是动态链接库和静态库的路径，使用cudnn的程序需要能够找到这些`.so`或`.a`库
多个路径以’:‘隔开
</code></pre>

<h3 id="toc_10">5. 安装 caffe</h3>

<h4 id="toc_11">5.1 安装依赖</h4>

<pre><code>yum install epel-realease

yum install -y protobuf-devel leveldb-devel snappy-devel opencv-devel boost-devel  hdf5-devel gflags-devel glog-devel lmdb-devel 

yum install -y openblas-devel
</code></pre>

<h4 id="toc_12">5.2 修改Makefile.config</h4>

<p>命令行运行<code>cp Makefile.config.example Makefile.config</code><br/>
修改以下内容</p>

<pre><code>USE_CUDNN := 1 # 编译GPU则去掉前面的注释
BLAS := open
BLAS_INCLUDE := /usr/include/openblas
BLAS_LIB := /usr/lib64

#需要安装matcaffe 则去掉注释，并修改为MATLAB路径
#MATLAB_DIR := /usr/local/MATLAB/R2016b  

#使用anaconda
#修改PYTHON_INCLUDE和PYTHON_LIB
#PYTHON_INCLUDE := $(ANACONDA_HOME)/include \
                 $(ANACONDA_HOME)/include/python2.7 \
                 $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \
#PYTHON_LIB := $(ANACONDA_HOME)/lib

#运行使用python自定义层,去掉前面的&#39;#&#39;注释
WITH_PYTHON_LAYER := 1

INCLUDE_DIRS := /usr/local/include /usr/include $(PYTHON_INCLUDE)
LIBRARY_DIRS := /usr/local/lib /usr/local/cudnn/lib64  $(PYTHON_LIB)
</code></pre>

<h4 id="toc_13">5.3 编译并测试</h4>

<pre><code>#编译并运行单元测试
make all -j
make runtest -j

#编译python接口，可选
make pycaffe -j  
make pytest  

#编译matlab接口，可选
make matcaffe -j
make mattest
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Double Jpeg detection with the same quantization matrix (Notes)]]></title>
    <link href="klauscc.github.io/14699724013434.html"/>
    <updated>2016-07-31T06:40:01-07:00</updated>
    <id>klauscc.github.io/14699724013434.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1. Introduction</h2>

<p>介绍一下几篇论文中等质量双压缩检测的方法，备忘。State of the-art double jepg compression detection with the same quantization matrix. </p>

<h2 id="toc_1">2. Methods</h2>

<h3 id="toc_2">2.1 Proposed by Fangjun et al.</h3>

<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560817">Detecting Double JPEG Compression With the Same Quantization Matrix</a> </p>

<h4 id="toc_3">2.1.1 jpeg压缩和解压缩过程导致的三种误差</h4>

<p>i. 量化误差 压缩过程中，DCT系数量化导致的误差<br/>
ii. 截断误差 解压缩过程中，IDCT变换可能导致值在[0,255]之外，需要截断到0或者255<br/>
iii. 舍入误差  解压缩， IDCT变换得到的值为浮点数，需要舍入为最接近的整数</p>

<h4 id="toc_4">2.1.2 jpeg多重压缩统计特征</h4>

<p>\(J_n\)代表压缩n次的Jpeg图片，\(D_n\)代表 \(J_n\)和\(J_{n+1}\)中JPEG系数（量化后的DCT系数）不一样的个数，\(S_n\)代表\(J_n\)中非0 JPEG系数的个数。<br/>
<strong>统计特征：</strong>随着n增加，\(D_n\)单调减小</p>

<span id="more"></span><!-- more -->

<h4 id="toc_5">2.1.3 检测方法</h4>

<ol>
<li>将原图片\(J\)压缩为\(J&#39;\),\(J\)和\(J&#39;\)的JPEG系数不一样的个数为\(D\)</li>
<li><p>然后修改\(J&#39;\)部分(修改比率为\(mpnc\))Jpeg系数，然后压缩得到\(J_m&#39;\)，将\(J_m&#39;\)再次压缩得到\(J_m&#39;&#39;\)。</p></li>
<li><p>\(J_m&#39;\)和\(J_m&#39;&#39;\)的JPEG系数不一样的个数为\(D_m\),重复n次取平均得到\(\bar{D}_m\)</p></li>
<li><p>判断依据<br/>
<center><br/>
\(\left\{<br/>
\begin{aligned}<br/>
if \bar{D}_m \ge D,~J~is~a~doubly~ compressed~image \\<br/>
if \bar{D}_m \le D,~J~is~a~singly~ compressed~image<br/>
\end{aligned}<br/>
\right.\)<br/>
</center></p></li>
</ol>

<h4 id="toc_6">2.1.4 实验</h4>

<p>在检测方法中，对准确率影响很重要的一个因素是\(mpnc\)。文章中是以固定步长0.001从0增加到0.118,同时检测准确率，获取准确率最高的\(mpnc\)。在预测其它数据集时，这个\(mpnc\)也有不错的泛化能力。<br/>
<strong>交叉检测：</strong>对三个数据集训练，得到最佳的\(mpnc\)为最佳值范围的中位数，然后用这个值来预测其它数据集的图片。<br/>
<strong>\(mpnc\)的影响：</strong> \(mpnc\)选取过大，会导致\(D_m\)过大；\(mpnc\)过小，会导致\(D_m\)过小。</p>

<h4 id="toc_7">2.1.5 讨论</h4>

<p><strong>为什么常数的\(mpnc\)适合不同的图片？</strong><br/>
对于不同的图片D差异很大。虽然\(mpnc\)一样，但是由它生成的\(D_m\)和图片相关，适合于不同的图片。</p>

<h3 id="toc_8">2.2 Proposed by Jianquan Yang et al.</h3>

<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6905821">An Effective Method for Detecting Double JPEG Compression With the Same Quantization Matrix</a></p>

<h4 id="toc_9">2.2.1 摘要</h4>

<p>基于误差统计特征的检测方法(error-based statistical feature extraction scheme):<br/>
1. 将图片\(J\)解压缩到空间域时，其IDCT系数与空间域像素值之差构成一个误差图片（error image,图片的每一个像素值为解压缩时对于IDCT系数与空间域值之差）。误差图片反应了图片解压缩过程中的截断误差(Truncation error)和舍入误差(Round error)。<br/>
2. 将error image分为一个个\(n*n\)的block(n=8,和DCT的block一样)。这些block可分为两类：\(n*n\)个值中至少存在一个截断误差，则为truncation block;否则为round block。<br/>
3. 每个block提取13维的特征，将这些特征使用SVM分类，然后预测即可得到很好的效果。</p>

<h4 id="toc_10">2.2.2 特征提取</h4>

<p><img src="https://lh3.googleusercontent.com/-wLiZ4-WcYUQ/V6iWYwrDMxI/AAAAAAAAD7c/Fw2LyJ1XE9c/I/14706662767265.jpg" alt=""/><br/>
1)压缩过n次的图片的error blocks值<center>\(R_n = RT(IDCT(\tilde{D}_n)) - IDCT(\tilde{D}_n)\)</center><br/>
\(K_{n+1}和K_n\)的关系为：<br/>
<center>\(K_{n+1} = [DCT(RT(IDCT(\tilde{D}_n)))/Q]\\<br/>
~~~~~~~= [DCT(IDCT(\tilde{D}_n)+R_n)/Q] \\<br/>
~~~~~~~= K_n + [DCT(R_n)/Q] \\<br/>
~~~~~~~=K_n+M_n\\<br/>
\)</center><br/>
结合上3式可以得到：<br/>
\(R_{n+1} = R_n + R_T ( IDCT( \tilde{D}_n ) + IDCT ( M_n × Q ) ) \\<br/>
~~~~~~~~~~~−RT(IDCT(\tilde{D}_n))− IDCT(M_n × Q)\)</p>

<p>如果\(M_n=0\)，则\(R_{n+1}=R_n\)。对于n-times压缩的error block，若\(M_{m+1}=0\)，则可认为该error block为useless（因为不会带来误差），称其为稳定的block;应该从提取特征的block中去除。<br/>
（对于rounding block, \(M_n\)很容易为0，因为每个元素取值范围为\([-0.5,0.5]\),其DCT变换后值小于8(\(F(0,0)&lt;4\))，如果\(Q(0,0)&gt;8\)和其它\(Q(u,v)&gt;16\)时M为0）<br/>
2）error-based statistical features(ESBF):<br/>
1. ESBF_spatial(4维): \(mean(|R_n^r|)\), \(var(|R_n^r|)\),\(mean(|R_n^t|)\), \(var(|R_n^t|)\) (截断误差和舍入误差的均值和方差)<br/>
2. EBSF_dct(8维): \(mean(|W_{n,D}^r|)\), \(var(|W_{n,D}^r|)\), \(mean(|W_{n,A}^r|)\), \(var(|W_{n,A}^r|)\), \(mean(|W_{n,D}^t|)\), \(var(|W_{n,D}^t|)\), \(mean(|W_{n,A}^t|)\), \(var(|W_{n,A}^t|)\) (A表示直流分量，D表示交流分量，r表示舍入误差，t表示截断误差)<br/>
3. EBSF_ratio(1维):  \(n_r/n_a\)  (\(n_r\)表示不稳定的rounding block 个数，\(n_a\)表示不稳定的block总个数）</p>

<h4 id="toc_11">2.2.3 训练</h4>

<p>将提取出的13维特征使用SVM训练，高斯核函数，cross-validation, grid-search.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Digital image forensics: a booklet for beginners]]></title>
    <link href="klauscc.github.io/14699720497183.html"/>
    <updated>2016-07-31T06:34:09-07:00</updated>
    <id>klauscc.github.io/14699720497183.html</id>
    <content type="html"><![CDATA[
<h3 id="toc_0">1. Introduction</h3>

<ul>
<li><strong>Digital image forensics(DIF)</strong>

<ul>
<li>two principal research paths:

<ol>
<li>attempt at answering question a)(device captured the image as declared), by <strong>performing some kind of ballistic analysis</strong> to identify the device that captured the image, or at least to determine which devices did not capture it. </li>
<li><strong>exposing traces of semantic manipulation</strong> (i.e. forgeries) by studying inconsistencies in natural image statistics. </li>
</ol></li>
</ul></li>
</ul>

<h3 id="toc_1">2. the role of digital image forensics in multimedia security</h3>

<ul>
<li><p>acquisition process and the tampering techniques leave subtle traces</p>

<ul>
<li><strong>active protection</strong>: watermarking</li>
</ul>

<h2 id="toc_2">- <strong>passive protection</strong>: tools in digital image forensics</h2>

<span id="more"></span><!-- more --></li>
</ul>

<h3 id="toc_3">3. image source device identification</h3>

<h4 id="toc_4">3.1 image acquisition and storage</h4>

<h5 id="toc_5">stage might introduce imperfections:</h5>

<ul>
<li>lens distortion, chromatic aberration, pixel defects or CCD sensor imperfections, statistical dependencies related to proprietary CFA interpolation algorithms and other intrinsic image regularities</li>
</ul>

<h4 id="toc_6">3.2  forensics methods for image source identification</h4>

<h5 id="toc_7">3.2.1 artifacts produced in the acquisition phase</h5>

<p><strong>lens aberration. lateral chromatic aberration analysis<br/>
demosaicing</strong>, introduces a specific type of correlation between the color value of one pixel and its neighboring samples in the same color channel.   so detect traces of CFA interpolation in color bands by using the <strong>expectation/maximization (EM) algorithm</strong>. </p>

<h5 id="toc_8">3.2.2 sensor imperfecitons</h5>

<p>sensor noise result from: pixel defects, fixed pattern noise(FPN), Photo Response Non Uniform(PRNU)</p>

<h5 id="toc_9">3.3.3 source identification using properties of imaging device</h5>

<h6 id="toc_10">post-processing performed in the storage phase</h6>

<ul>
<li>image features: color-related measurements,image quality features</li>
<li>identification of the quantization table(JPEG compress)

<ul>
<li><strong>image thumbnails</strong>( it involves filtering operation, contrast adjustment,jpeg compression, depends on imaging device manufacturer and device model)</li>
</ul></li>
</ul>

<h3 id="toc_11">4. tampering detection</h3>

<h4 id="toc_12">4.1 forgeries:</h4>

<ol>
<li>remove information   in-painting methods, seam carving method</li>
<li>add information   Blending and matting techniques;  Rotation, scaling and translation </li>
</ol>

<h4 id="toc_13">4.2 methods for forgery detection</h4>

<h5 id="toc_14">4.2.1 detecting tampering performed ion a single image</h5>

<ul>
<li><p>copy-move of an image region，the tampered area still shares most of its intrinsic properties (e.g. pattern noise or color palette) </p>

<ol>
<li>[34] proposed to look for matches among DCT representations of image segments. </li>
<li>[78] perform a principal component analysis(PCA) for the description of image segments
exploits SIFT features, represent image segments in a Fourier-Mellin Transform domain.  to obtain robust to scaling and rotation operations</li>
</ol></li>
<li><p>to detect forgeries by in-painting and content-aware resizing techniques</p>

<ol>
<li>[83]observe that seam carving introduces inconsistencies in the image high frequency DCT components </li>
<li>[33] extracting energy-bias-based features and wavelet absolute moments. </li>
</ol></li>
</ul>

<h5 id="toc_15">4.2.2 detecting image composition</h5>

<ul>
<li>Bicoherence features [27]</li>
<li>incident light direction[46][48]<br/></li>
</ul>

<h5 id="toc_16">4.2.3 tampering detection independent on the type of forgery</h5>

<ul>
<li><p>three different types of artifacts: traces of re-sampling, compression artifacts and inconsistencies in acquisition device fingerprints. </p></li>
<li><p>to detect re-sampling process, which produces correlation among the image pixel, [80] use the Expectation/ Maximization algorithm to estimate the probability of each pixel to be correlated with its neighbors </p>

<ul>
<li>[31]another method is to check if diferrent zones of the image have diferrent “compression histories”</li>
</ul></li>
<li><p>a more general scenario: <strong>double compression</strong></p>

<ul>
<li>it introduces specific artifacts in the DCT coefficient histograms </li>
<li>that represent important cues in tampering detection. </li>
<li>[77] proposed the first theoretical analysis ,mainly due to the effects of double quantization: \(s_{ab}[t] = q_{ab}(u) = \lfloor\lfloor \frac{u}{b}\rfloor \frac{b}{a}\rfloor\)</li>
</ul></li>
</ul>

<h3 id="toc_17">5. A new phase: counter-forensics</h3>

<h4 id="toc_18">5.1 tamper hiding</h4>

<ul>
<li>[54]hiding traces of region re-sampling targeted on [80] which exposes forgeries by detecting the linear dependencies that re-sampling typically induces among pixels. <strong>post-processing the image with a median(non linear) filter, geometric distortions</strong></li>
</ul>

<h4 id="toc_19">5.2 image source counterfeiting</h4>

<ul>
<li>[39] attempt to fool source device identification method[65],which is based on the extraction and the analysis of the camera pattern noise </li>
<li>[39] use flat-fielding to estimate both the FPN and the PRNU for the device of interest. 

<ul>
<li>pattern noise can be eventually suppressed from image x by:       \(J = \frac{I- d}{K}\)</li>
<li>another device in terms of \((d_e, K_e)\) : \(J_{cont} = J* K_e +d_e\)</li>
<li>forged in the polished image to counterfeit the camera signature</li>
</ul></li>
</ul>

<h4 id="toc_20">5.3 countering counter-forensics</h4>

<h3 id="toc_21">6. conclusions</h3>

<h4 id="toc_22">concern:</h4>

<ol>
<li>the robustness of the existing tools</li>
<li>confirming or strengthening the robustness of DIF techniques is a present priority for DIF experts </li>
<li>the development of counter-forensics is to be encouraged</li>
<li>future efforts in DIF should be also addressed towards video authentication. </li>
<li>the major challenge in the future of image forensics consists in integrating it with visual perception. </li>
</ol>

]]></content>
  </entry>
  
</feed>
